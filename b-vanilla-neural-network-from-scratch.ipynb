{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CggyzPOvQ8d"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/apssouza22/cnn-for-devs/blob/master/b-vanilla-neural-network-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JTfVbH7FHUx"
   },
   "source": [
    "# Implementing a vanilla Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCsjDO-bvIqp"
   },
   "source": [
    "\n",
    "\n",
    "In this section, we will learn how to implement the backpropagation algorithm from scratch using Python. \n",
    "\n",
    "**What is Backpropagation?**\n",
    "Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors.\" It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.\n",
    "\n",
    "The backpropagation algorithm consists of two phases:\n",
    "- 1. The forward pass where we pass our inputs through the network to obtain our output classifications.\n",
    "\n",
    "-  2. The backward pass (i.e., weight update phase) where we compute the gradient of the loss function and use this information to iteratively apply the chain rule to update the weights in our network.\n",
    "\n",
    "<img src=\"https://www.guru99.com/images/1/030819_0937_BackPropaga1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2DrYmkG701Je"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFd2HULUWrDo"
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFsuJylxWrDp"
   },
   "source": [
    "Before we dive into our neural network implementation, lets look at some important concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSZzUBVPWrDp"
   },
   "source": [
    "## The dot product\n",
    "\n",
    "The dot product of two vectors tells you how similar they are in terms of direction and is scaled by the magnitude of the two vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cE_RvheWrDq",
    "outputId": "543a3839-e578-4fc4-a38d-592f26070b67"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The dot product 1 is: 2.1672\n",
      "The dot product 2 is: 4.1259999999999994\n",
      "The dot product 2 is closer: 4.1259999999999994 > 2.1672\n"
     ]
    }
   ],
   "source": [
    "input_vector = [1.72, 1.23]\n",
    "weights_1 = [1.26, 0]\n",
    "weights_2 = [2.17, 0.32]\n",
    "\n",
    "# Computing the dot product of input_vector and weights_1\n",
    "first_indexes_mult = input_vector[0] * weights_1[0]\n",
    "second_indexes_mult = input_vector[1] * weights_1[1]\n",
    "dot_product_1 = first_indexes_mult + second_indexes_mult\n",
    "\n",
    "print(f\"The dot product 1 is: {dot_product_1}\")\n",
    "\n",
    "#use np instead\n",
    "dot_product_2 = np.dot(input_vector, weights_2)\n",
    "print(f\"The dot product 2 is: {dot_product_2}\")\n",
    "print(f\"The dot product 2 is closer: {dot_product_2} > {dot_product_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EuHSBJuWrDr"
   },
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "The fundamental operations of any typical neural network can be reduced to a bunch of addition and multiplication operations. Neural networks can be expressed in terms of matrices. Matrix multiplication is one of the most important mathematical operations when it comes to deep neural networks.\n",
    "\n",
    "Below a very simple \"neural net\" for helping to understand matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayYEuwR5WrDs",
    "outputId": "5421f15b-58e7-4b37-c5e5-426d368fe488"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0]  ->  [[0]]\n",
      "[1, 0]  ->  [[1]]\n",
      "[0, 1]  ->  [[1]]\n",
      "[1, 1]  ->  [[0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#weights for 2 neurons\n",
    "weights_for_layer1 = np.array([\n",
    "\t[1.5, \t-0.5],\n",
    "\t[-1, \t   1],\n",
    "\t[-1, \t   1]\n",
    "])\n",
    "\n",
    "#weights for 1 neurons\n",
    "weights_for_layer2 = np.array([\n",
    "\t[-1],\n",
    "\t[1],\n",
    "\t[1]\n",
    "])\n",
    "\n",
    "def activation_function(x):\n",
    "\treturn np.where(x>0, 1, 0)\n",
    "\n",
    "def neural_net(inputs, layer_weights, activation_function):\n",
    "\toutputs = inputs\n",
    "\t#The output of a layer become the input for the subsequent layer\n",
    "\tfor weight in layer_weights:\n",
    "\t\tbias = np.ones(shape=(outputs.shape[0], 1)) #adding 1 as a bias as an extra column\n",
    "\t\tinputs = np.hstack([bias, outputs])\n",
    "\t\tmatrixMultiplied= np.matmul(inputs, weight)\n",
    "\t\toutputs = activation_function(matrixMultiplied)\n",
    "\treturn outputs\n",
    "\n",
    "inputs = [\n",
    "\t[0, 0],\n",
    "\t[1, 0],\n",
    "\t[0, 1],\n",
    "\t[1, 1]\n",
    "]\n",
    "\n",
    "for i in inputs:\n",
    "\tprint(\n",
    "\t\ti,\n",
    "\t\t\" -> \",\n",
    "\t\tneural_net(\n",
    "\t\t\tinputs=np.array([i]),\n",
    "\t\t\tlayer_weights=[weights_for_layer1, weights_for_layer2],\n",
    "\t\t\tactivation_function=activation_function\n",
    "\t\t)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM9aKUkvWrDs"
   },
   "source": [
    "# Implementing a Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQgU_Z8q1JeB"
   },
   "source": [
    "A neural network is compound by multiple layers.\n",
    "\n",
    "Every layer will have a forward pass and backpass implementation. Let's create a main class layer which can do a forward pass .forward() and Backward pass .backward().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lD0VYK4I1nTN"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here we can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If our layer has parameters (e.g. linear layer), we also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3Xsi8DyvIB3"
   },
   "source": [
    "## Activation function (Relu)\n",
    "In a neural network, the purpose of an activation function is to add non-linearity to the neural network.\n",
    "\n",
    "They allow backpropagation because now the derivative function would be related to the input, and it’s possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
    "\n",
    "They allow the stacking of multiple layers of neurons as the output would now be a non-linear combination of input passed through multiple layers. Any output can be represented as a functional computation in a neural network.\n",
    "\n",
    "The rectified linear activation function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less.\n",
    "\n",
    "```\n",
    "if input > 0:\n",
    "\treturn input\n",
    "else:\n",
    "\treturn 0\n",
    "```\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VKn2K0Ud0Qca"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        relu_forward = np.maximum(0,input)\n",
    "        return relu_forward\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss with respect to ReLU input\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output * relu_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7_1NJKN748m"
   },
   "source": [
    "### Linear layer\n",
    "\n",
    "Now let's build something more complicated. Unlike nonlinearity, a linear layer actually has something to learn.\n",
    "\n",
    "A linear layer applies affine transformation. In a vectorized form, it can be described as:\n",
    "$$f(X)= W \\cdot X + \\vec b $$\n",
    "\n",
    "Both W and b are initialized during layer creation and updated each time backward is called. Note that we are using **Xavier initialization** which is a trick to train our model to converge faster [read more](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization). Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T19:04:12.511355-05:00",
     "start_time": "2018-06-03T19:04:12.492305Z"
    },
    "id": "ksQ6fD_Z748m"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A linear layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_size+output_size)), \n",
    "                                        size = (input_size,output_size))\n",
    "        self.biases = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Perform forward activation: f(x) = <W*x> + b\n",
    "        \n",
    "        input shape: [batch, input_size]\n",
    "        output shape: [batch, output_size]\n",
    "        \"\"\"\n",
    "        input = img.reshape([img.shape[0], -1])\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient with respect to weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. Updating weights and biases to all layers in the network\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k4MyzE9748n"
   },
   "source": [
    "### The loss function\n",
    "\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T19:04:47.947883-05:00",
     "start_time": "2018-06-03T19:04:47.939333Z"
    },
    "id": "5su5_htL748n"
   },
   "outputs": [],
   "source": [
    "def crossentropy_loss(predictions, labels):\n",
    "\t\"\"\"Calculate the softmax cross entropy loss for the predictions\"\"\"\n",
    "\tpredictions_for_answers = predictions[np.arange(len(predictions)), labels]\n",
    "\txentropy = - predictions_for_answers + np.log(np.sum(np.exp(predictions), axis=-1))\n",
    "\treturn xentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOqtHXxmIf98"
   },
   "source": [
    "## Why derivative/gradient is used ?\n",
    "\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.\n",
    "\n",
    "The error is given by the y-axis. If you’re in point A and want to reduce the error toward 0, then you need to bring the x value down. On the other hand, if you’re in point B and want to reduce the error, then you need to bring the x value up. To know which direction you should go to reduce the error, you’ll use the derivative. \n",
    "\n",
    "<img src=\"https://files.realpython.com/media/quatratic_function.002729dea332.png\" width=\"500px\">\n",
    "\n",
    "The gradient is a vector; it points in the direction and derivative is a rate of change of , which can be thought of the slope of the function at a point .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eD8__mJnIiUC"
   },
   "outputs": [],
   "source": [
    "def crossentropy_loss_grad_derivative(predictions, labels):\n",
    "\t\"\"\"Calculate the direction rate of the slope\"\"\"\n",
    "\tones_for_answers = np.zeros_like(predictions)\n",
    "\tones_for_answers[np.arange(len(predictions)), labels] = 1\n",
    "\n",
    "\tsoftmax = np.exp(predictions) / np.exp(predictions).sum(axis=-1, keepdims=True)\n",
    "\n",
    "\treturn (- ones_for_answers + softmax) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shEEaQxS3tO8"
   },
   "source": [
    "## Learning rate\n",
    "\n",
    "Is the step size at each iteration while moving toward a minimum of a loss function. Learning rate can not be too small or too big. \n",
    "\n",
    "\n",
    "<img src=\"https://srdas.github.io/DLBook/DL_images/TNN2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9hJvYimE6K_"
   },
   "source": [
    "# Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyOk3mmp748n"
   },
   "source": [
    "## Loading the dataset\n",
    "Now let's combine what we've just built into a working neural network. As I have told earlier, we are going to use MNIST data of handwritten digit for our example.\n",
    "\n",
    "\n",
    "This subset of the MNIST dataset is built-into the scikit-learn library and includes 1,797 example digits, each of which are 8 × 8 grayscale images (the original images are 28 × 28. When flattened, these images are represented by an 8 × 8 = 64-dim vector.\n",
    "\n",
    "<img src=\"https://nvsyashwanth.github.io/machinelearningmaster/assets/images/digitsMNIST/samples.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpl9LZQ7Ej87",
    "outputId": "f6080595-5d12-4290-8f83-2e0712ffd153"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] loading MNIST (sample) dataset...\n",
      "[INFO] samples: 1797, dim: 64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_dataset(flatten=False):\n",
    "    # load the MNIST dataset and apply min/max scaling to scale the\n",
    "    # pixel intensity values to the range [0, 1] (each image is\n",
    "    # represented by an 8 x 8 = 64-dim feature vector)\n",
    "    print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "    digits = datasets.load_digits()\n",
    "    data = digits.data.astype(\"float\")\n",
    "    data = (data - data.min()) / (data.max() - data.min()) # normalization\n",
    "    print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],data.shape[1]))\n",
    "    #X = data, Y = label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.25)\n",
    "\n",
    "    # we reserve the last 300 training examples for validation\n",
    "    X_train, X_val = X_train[:-300], X_train[-300:]\n",
    "    y_train, y_val = y_train[:-300], y_train[-300:]\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training loop\n",
    "\n",
    "We split data into minibatches, feed each such minibatch into the network and update weights. This training method is called a mini-batch stochastic gradient descent."
   ],
   "metadata": {
    "id": "GzZfmc9SkO0Z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:44:01.727147-05:00",
     "start_time": "2018-06-03T18:44:01.719126Z"
    },
    "id": "-chb7zrr748p"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "        \n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating our network model"
   ],
   "metadata": {
    "id": "duseU63wqyfc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NeuralNetwork:\n",
    "\tdef __init__(self, learning_rate=0.1):\n",
    "\t\t# [input_size, 32, 16, 10]       \n",
    "    # We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial. \n",
    "\t\tself.network = []\n",
    "\t\tself.network.append(Linear(64, 100, learning_rate))\n",
    "\t\tself.network.append(ReLU())\n",
    "\t\tself.network.append(Linear(100, 200, learning_rate))\n",
    "\t\tself.network.append(ReLU())\n",
    "\t\tself.network.append(Linear(200, 10, learning_rate))\n",
    "\n",
    "\tdef forward(self, img):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute activations of all network layers by applying them sequentially.\n",
    "\t\tReturn a list of activations for each layer. \n",
    "\t\t\"\"\"\n",
    "\t\tpredictions = []\n",
    "\t\tinput = img\n",
    "\n",
    "\t\t# Looping through each layer\n",
    "\t\tfor l in self.network:\n",
    "\t\t\tpredictions.append(l.forward(input))\n",
    "\t\t\t# Updating input to last layer output\n",
    "\t\t\tinput = predictions[-1]\n",
    "\n",
    "\t\tassert len(predictions) == len(self.network)\n",
    "\t\treturn predictions\n",
    "\n",
    "\tdef predict(self, img):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute network predictions. Returning indices of largest probability\n",
    "\t\t\"\"\"\n",
    "\t\tprediction = self.forward(img)[-1]\n",
    "\t\treturn prediction.argmax(axis=-1)\n",
    "\n",
    "\tdef train(self, epoch, img_train, label_train, img_val, label_val):\n",
    "\t\t\"\"\"\n",
    "\t\tTrain our network on a given batch of X and y.\n",
    "\t\tWe first need to run forward to get all layer activations.\n",
    "\t\tThen we can run layer.backward going from last to first layer.\n",
    "\t\tAfter we have called backward for all layers, all Linear layers have already made one gradient step.\n",
    "\t\t\"\"\"\n",
    "\t\ttrain_logs = []\n",
    "\t\tval_logs = []\n",
    "\t\tfor epoch in range(epoch):\n",
    "\t\t\tfor x_batch, y_batch in iterate_minibatches(img_train, label_train, batchsize=32, shuffle=True):\n",
    "\t\t\t\tloss, loss_grad, layer_inputs = self.training_step(x_batch, y_batch)\n",
    "\t\t\t\tself.backward(layer_inputs, loss_grad)\n",
    "\n",
    "\t\t\ttrain_log, val_log = self.validation_epoch_end(img_train, label_train, img_val, label_val)\n",
    "\t\t\ttrain_logs.append(train_log)\n",
    "\t\t\tval_logs.append(val_log)\n",
    "\n",
    "\t\treturn train_logs, val_logs\n",
    "\n",
    "\tdef training_step(self, img, label):\n",
    "\t\t# Get the layer activations\n",
    "\t\tlayer_predictions = self.forward(img)\n",
    "\t\tlayer_inputs = [img] + layer_predictions  # layer_input[i] is an input for network[i]\n",
    "\t\tlast_prediction = layer_predictions[-1]\n",
    "\n",
    "\t\t# Compute the loss and the initial gradient\n",
    "\t\tloss = crossentropy_loss(last_prediction, label)\n",
    "\t\tloss_grad = crossentropy_loss_grad_derivative(last_prediction, label)\n",
    "\n",
    "\t\treturn np.mean(loss), loss_grad, layer_inputs\n",
    "\n",
    "\tdef validation_epoch_end(self, img_train, label_train, img_val, label_val):\n",
    "\t\ttrain_log = np.mean(self.predict(img_train) == label_train)\n",
    "\t\tval_log = np.mean(self.predict(img_val) == label_val)\n",
    "\t\treturn val_log, train_log\n",
    "\t\n",
    "\tdef backward(self, layer_inputs, loss_grad):\n",
    "\t\t# Propagate gradients through the network\n",
    "\t\t# Reverse loop as this is backprop\n",
    "\t\tfor layer_index in range(len(self.network))[::-1]:\n",
    "\t\t\tlayer = self.network[layer_index]\n",
    "\t\t\tlayer_input = layer_inputs[layer_index]\n",
    "\t\t\tloss_grad = layer.backward(layer_input, loss_grad)  # grad w.r.t. input, also weight updates\n",
    "\n",
    "\n",
    "model = NeuralNetwork(learning_rate=0.1)\n"
   ],
   "metadata": {
    "id": "qWWJM69RXDuN"
   },
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:47:51.387863-05:00",
     "start_time": "2018-06-03T18:47:51.384341Z"
    },
    "id": "L_zo8T4V748p"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:49:20.383230-05:00",
     "start_time": "2018-06-03T18:47:53.604185Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "0zhhjgoR748q",
    "outputId": "e2fc1614-f29f-4f50-ef87-d641c9bb794b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy: 0.9633333333333334\n",
      "Val accuracy: 0.997134670487106\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "epoch=30\n",
    "train_log, val_log = model.train(epoch, X_train, y_train, X_val, y_val)\n",
    "clear_output()\n",
    "print(\"Train accuracy:\", train_log[-1])\n",
    "print(\"Val accuracy:\", val_log[-1])\n",
    "plt.plot(train_log, label='train accuracy')\n",
    "plt.plot(val_log, label='val accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgxZze99748q"
   },
   "source": [
    "As we can see we have successfully trained a MLP which was purely written in numpy with high validation accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zum-oFsFQNCW"
   },
   "source": [
    "## Classification report.\n",
    "\n",
    "Look at the precision column and you will see the percentage of right prediction to each number. The closer to 1 the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5fFtXfKOWCm",
    "outputId": "8bfdf77d-1e40-458d-c658-10ec82d7db28"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       103\n",
      "           1       0.99      1.00      1.00       101\n",
      "           2       1.00      1.00      1.00        98\n",
      "           3       1.00      1.00      1.00       108\n",
      "           4       1.00      1.00      1.00       113\n",
      "           5       1.00      1.00      1.00       109\n",
      "           6       1.00      1.00      1.00       105\n",
      "           7       1.00      1.00      1.00       114\n",
      "           8       1.00      0.99      1.00       101\n",
      "           9       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00      1047\n",
      "   macro avg       1.00      1.00      1.00      1047\n",
      "weighted avg       1.00      1.00      1.00      1047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(testData, testLabel):\n",
    "  # the predict expect a list of imgs that is why we need np.expand_dims\n",
    "    predictions = [model.predict(np.expand_dims(item, axis=0)) for item in testData]\n",
    "    print(classification_report(testLabel, predictions))\n",
    "\n",
    "evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjhEtuSvQXvn"
   },
   "source": [
    "## Random test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyE43GXZQfpk",
    "outputId": "23e4b717-3a81-4045-b235-49c19fcfcb74"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Test network with  2\n",
      "Result =  [2]\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Test network with \", y_train[0])\n",
    "predictions = model.predict(np.expand_dims(X_train[0], axis=0))\n",
    "print(\"Result = \", predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Building_neural_network_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (opencv-projects)",
   "language": "python",
   "name": "pycharm-4eccda7c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}