{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbLC7wcIcK4"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/apssouza22/cnn-for-devs/blob/master/f-image-interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oZyY8beAwWo"
   },
   "source": [
    "Image interpolation with Autoencoders\n",
    "======\n",
    "\n",
    "\n",
    "We are going to learn how to work with autoencoders to generate Latent Space/ feature map and from there do some nice things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGWf_LzX6ZoM"
   },
   "source": [
    "Autoencoder Definition\n",
    "-----------------------\n",
    "AutoEncoder is an unsupervised Artificial Neural Network that attempts to encode the data by compressing it into the lower dimensions(dimensionality reduction) and then decoding the data to reconstruct the original input.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*AJ_zWZundfZ2-11eZUw5GA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QeU9mB0_AwWs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameter Settings\n",
    "-------------------\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "use_gpu = True\n",
    "img_size = 64\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flbWQTlZAwWu"
   },
   "source": [
    "MNIST Data Loading\n",
    "-------------------\n",
    "\n",
    "MNIST images show digits from 0-9 in 28x28 grayscale images. We normalize and center them around 0, which gives a slight performance boost during training.\n",
    "We create both a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6uK2neSDAwWv"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Autoencoder consists of an Encoder and a Decoder model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkY2beRZAwWw",
    "outputId": "607c238d-831c-483e-d467-6c5759c258ac"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of parameters: 396171\n"
     ]
    }
   ],
   "source": [
    "# Model responsible for generating our Latent space.\n",
    "#Latent space, Typically it is a 100-dimensional hypersphere with each variable drawn from a Gaussian distribution with a mean of zero and a standard deviation of one\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=img_size, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(in_channels=img_size, out_channels=img_size*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
    "        self.fc = nn.Linear(in_features=img_size * 2 * 7 * 7, out_features=num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Model responsible for reconstruct the image form our Latent space.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=num_classes, out_features=img_size*2*7*7)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=img_size * 2, out_channels=img_size, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=img_size, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), img_size * 2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered\n",
    "        return x\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon\n",
    "    \n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "#The modelâ€™s parameters that need to be trained.\n",
    "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE5kxqxuAwWx"
   },
   "source": [
    "Train Autoencoder\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "ErtOYe8SAwWy",
    "outputId": "15324405-d102-482d-e212-eb24718efdab",
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileExistsError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileExistsError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-2c675d5b880d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmkdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"assets\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mENCODER_MODEL_PATH\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"assets/encoder.pth\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mDECODER_MODEL_PATH\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"assets/decoder.pth\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileExistsError\u001B[0m: [Errno 17] File exists: 'assets'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"assets\")\n",
    "\n",
    "ENCODER_MODEL_PATH=\"assets/encoder.pth\"\n",
    "DECODER_MODEL_PATH=\"assets/decoder.pth\"\n",
    "\n",
    "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "num_epochs=30\n",
    "\n",
    "# set to training mode\n",
    "autoencoder.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        image_batch = image_batch.to(device)\n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "      \n",
    "        # reconstruction error\n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    torch.save(autoencoder.encoder.state_dict(), ENCODER_MODEL_PATH)\n",
    "    torch.save(autoencoder.decoder.state_dict(), DECODER_MODEL_PATH)\n",
    "\n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0dFn3Z3AwWy"
   },
   "source": [
    "Plot Training Curve\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LX8glP0EAwWz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Reconstruction error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzgXndbGAwW0"
   },
   "source": [
    "Evaluate on the Test Set\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fwptU4XAwW0"
   },
   "outputs": [],
   "source": [
    "# set to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "test_loss_avg, num_batches = 0, 0\n",
    "for image_batch, _ in test_dataloader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        image_batch = image_batch.to(device)\n",
    "\n",
    "        # autoencoder reconstruction\n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "\n",
    "        # reconstruction error\n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "\n",
    "        test_loss_avg += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "test_loss_avg /= num_batches\n",
    "print('average reconstruction error: %f' % (test_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbNtBp8XIcLD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "plt.ion()\n",
    "\n",
    "# Util function to display the images\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = model(images)\n",
    "        images = images.cpu()\n",
    "        images = to_img(images)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGOZhBrEAwW1"
   },
   "source": [
    "Visualize Reconstructions\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1u5HgMJAwW1"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "images, labels = iter(test_dataloader).next()\n",
    "\n",
    "# First visualise the original images\n",
    "print('Original images')\n",
    "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
    "plt.show()\n",
    "\n",
    "# Reconstruct and visualise the images using the autoencoder\n",
    "print('Autoencoder reconstruction:')\n",
    "visualise_output(images, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgg-LOtFw7WO"
   },
   "outputs": [],
   "source": [
    "# Group all digits in a map ditits[i]=[...]\n",
    "digits = [[] for _ in range(10)]\n",
    "for img_batch, label_batch in test_dataloader:\n",
    "    for i in range(img_batch.size(0)):\n",
    "        digits[label_batch[i]].append(img_batch[i:i+1])\n",
    "\n",
    "    if sum(len(d) for d in digits) >= 1000:\n",
    "        break;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v6ACt9AAwW1"
   },
   "source": [
    "Interpolate in Latent Space\n",
    "----------------------------\n",
    "\n",
    "The latent representation of autoencoders have been studied in the context of enabling interpolation between data points by decoding convex combinations of latent vectors\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/480/1*vEZE5VcjUr5RUbt_OWfR_w.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRsREYNaAwW1"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "def interpolation(lambda1, model, img1, img2):\n",
    "    with torch.no_grad():\n",
    "        # latent vector of first image\n",
    "        img1 = img1.to(device)\n",
    "        latent_1 = model.encoder(img1)\n",
    "\n",
    "        # latent vector of second image\n",
    "        img2 = img2.to(device)\n",
    "        latent_2 = model.encoder(img2)\n",
    "\n",
    "        # interpolation of the two latent vectors\n",
    "        inter_latent = lambda1 * latent_1 + (1- lambda1) * latent_2\n",
    "\n",
    "        # reconstruct interpolated image\n",
    "        inter_image = model.decoder(inter_latent)\n",
    "        inter_image = inter_image.cpu()\n",
    "    \n",
    "    return inter_image\n",
    "    \n",
    "\n",
    "# interpolation lambdas\n",
    "lambda_range=np.linspace(0,1,10)\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# We start off by taking two images from the dataset, and linearly interpolate between them\n",
    "for ind,l in enumerate(lambda_range):\n",
    "    inter_image = interpolation(float(l), autoencoder, digits[7][0], digits[1][0])\n",
    "    inter_image = to_img(inter_image)\n",
    "    image = inter_image.numpy()\n",
    "   \n",
    "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
    "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAt4HVuZAwW2"
   },
   "source": [
    "Random Latent Vector - generating random images\n",
    "-------------------------------------------------\n",
    "\n",
    "Typically, new images are generated using random points in the latent space. Taken a step further, points in the latent space can be constructed (e.g. all 0s, all 0.5s, or all 1s) and used as input or a query to generate a specific image.\n",
    "\n",
    "We are going to transform our latent space by using simple vector arithmetic to create new points in the latent space that, in turn, can be used to generate images. This is an interesting idea, as it allows for the intuitive and targeted generation of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52NHPksQAwW2"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # approx. fit a multivariate Normal distribution (with diagonal cov.) to the latent vectors of a random part of the test set\n",
    "    images, labels = iter(test_dataloader).next()\n",
    "    images = images.to(device)\n",
    "    latent = autoencoder.encoder(images)\n",
    "    latent = latent.cpu()\n",
    "\n",
    "    # Finding the mean and standard deviation from our latent space\n",
    "    mean = latent.mean(dim=0)\n",
    "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
    "\n",
    "    # sample latent vectors from the normal distribution\n",
    "    latent = torch.randn(latent.shape[0], latent.shape[1])*std + mean\n",
    "\n",
    "    # reconstruct images from the latent vectors\n",
    "    latent = latent.to(device)\n",
    "    img_recon = autoencoder.decoder(latent)\n",
    "    img_recon = img_recon.cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    " latent = torch.randn(128, num_classes)*std + mean\n",
    " latent"
   ],
   "metadata": {
    "id": "Roanl84aSJDu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "p-KtxeuuSJ1o"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}